安装，build和使用DataX:

https://github.com/alibaba/DataX/blob/master/userGuid.md

python /opt/datax/bin/datax.py xx.json

解压之后的内容主要是在/bin当中,/script当中是readme的内容

hduser@master:/opt/datax/job$ python /opt/datax/bin/datax.py job.json
=====================================================

生成Job的Json文件
python datax.py -r {YOUR_READER} -w {YOUR_WRITER}
EG: python /opt/datax/bin/datax.py -r streamreader -w streamwriter

https://github.com/alibaba/DataX
READER和WRITER就是代码仓库当中的那些reader和writer

python /opt/datax/bin/datax.py stream2stream.json
会打印出50次   hello，你好，世界-DataX
=======================================================
https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md
这个例子将Mysql当中的元素取出，并且在stream上输出出来

https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md
这个例子是将数据写入到hdfs当中

hive conf当中有hive-site.conf这个配置文件当中会指定hive在hdfs上保存文件的位置。
/user/hive/warehouse/

https://cwiki.apache.org/confluence/display/Hive/LanguageManual


create table text_table(
col1  SMALLINT,
col2  VARCHAR(255),
col3  VARCHAR(255)
)
row format delimited
fields terminated by "\t"
STORED AS TEXTFILE;

参考mysql2hdfs中的内容

===========================================================
Job: Job是DataX用以描述从一个源头到一个目的端的同步作业，是DataX数据同步的最小业务单元。比如：从一张mysql的表同步到odps的一个表的特定分区。
Task: Task是为最大化而把Job拆分得到的最小执行单元。比如：读一张有1024个分表的mysql分库分表的Job，拆分成1024个读Task，用若干个并发执行。
TaskGroup: 描述的是一组Task集合。在同一个TaskGroupContainer执行下的Task集合称之为TaskGroup
JobContainer: Job执行器，负责Job全局拆分、调度、前置语句和后置语句等工作的工作单元。类似Yarn中的JobTracker
TaskGroupContainer: TaskGroup执行器，负责执行一组Task的工作单元，类似Yarn中的TaskTracker。

Standalone: 单进程运行，没有外部依赖。
Local: 单进程运行，统计信息、错误信息汇报到集中存储。
Distrubuted: 分布式多进程运行，依赖DataX Service服务。

目前主要有三类脏数据：

Reader读到不支持的类型、不合法的值。
不支持的类型转换，比如：Bytes转换为Date。
写入目标端失败，比如：写mysql整型长度超长。

看两张图：图1：运行顺序； 图2：依赖顺序

重编plugin，在项目总目录下运行下列的命令
mvn clean package -DskipTests assembly:assembly

==============================================================
再dataX.py当中查找startCommand，将其取消注释，可以看到究竟是怎样启动DataX的
python ../bin/datax.py ./mysql2stream.json > ~/cmd.txt

java -server -Xms1g -Xmx1g 
-XX:+HeapDumpOnOutOfMemoryError 
-XX:HeapDumpPath=/opt/datax/log -Xms1g -Xmx1g 
-XX:+HeapDumpOnOutOfMemoryError 
-XX:HeapDumpPath=/opt/datax/log 
-Dloglevel=info -Dfile.encoding=UTF-8 
-Dlogback.statusListenerClass=ch.qos.logback.core.status.NopStatusListener 
-Djava.security.egd=file:///dev/urandom -Ddatax.home=/opt/datax 
-Dlogback.configurationFile=/opt/datax/conf/logback.xml -classpath /opt/datax/lib/*:.  
-Dlog.file.name=ob_mysql2stream_json 
com.alibaba.datax.core.Engine -mode standalone -jobid -1 -job /opt/datax/job/mysql2stream.json

dataX本身有一个配置是在  conf/core.json

然后每个plugin也有json配置文件，最后的配置文件应该是将三个配置文件加在一起来使用的（todo待验证）

最后一行的三个参数mode,jobid,job是会传到java当中去的，core/Engine当中会解析这几个参数
然后会调用Engine.start()，在start当中会有：
        // 绑定column转换信息
        ColumnCast.bind(allConf);

        /**
         * 初始化PluginLoader，可以获取各种插件配置
         */
        LoadUtil.bind(allConf);
这两个方法，会将column 转换和plugin加载的工作完成

jobContainer里面有个initStandaloneScheduler，会保存下containerCommunicator

最后完成JobContainer的初始化之后，会调用container.start将container跑起来
分以下几个阶段

preHandle:默认配置下为空，不需要关注
-------------------------------------------
init：初始化reader和writer

classLoaderSwapper完成plugin类的热加载

DefaultJobPluginCollector   会使用这个去初始化一些plugin之类的东西，在一开始的时候里面的那个Communicator是空的，
2019-02-12 23:23:46.997 [job-0] INFO  JobContainer - Xiaogang: reader init
mysqlreader
{"column":["film_id","title","description"],"connection":[{"jdbcUrl":["jdbc:mysql://127.0.0.1:3306/sakila"],
"table":["film"]}],"password":"123","splitPk":"film_id","username":"root"}
{"print":true}
null

以上是用parameter的配置加到reader和writer当中，而Commnicater实际上是空的。会在后面才完成初始化，应该是用来读取配置的

classLoaderSwapper.setCurrentThreadClassLoader(LoadUtil.getJarLoader(
        PluginType.READER, this.readerPluginName));
classLoaderSwapper.restoreCurrentThreadClassLoader();

每次运行一次reader,writer中的某个阶段，就要切换一下环境。
-----------------------------------------------
prepare:
进行prepare阶段，貌似mysql和stream的prepare都为空
-----------------------------------------------
split:
adjustChannelNumber： 当中一次对于byte,record进行限制，如果都没有设置，至少要针对Channel的速度进行限制，
拿当前的例子，就是返回了一个channel的number，用于后续计算reader数量的时候算出来一个adviceNumber











